{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90cd4a2-26f4-48cd-ba47-7d3dc9e3d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMost common words in reviews:\u001b[0m\n",
      "     Word  Count\n",
      "0  guitar    665\n",
      "1    pick    503\n",
      "2   sound    393\n",
      "3    good    371\n",
      "4   great    353\n",
      "\n",
      "\n",
      "\u001b[1mMost common words in summaries:\u001b[0m\n",
      "     Word  Count\n",
      "0   great    159\n",
      "1    good    138\n",
      "2    pick     85\n",
      "3  guitar     66\n",
      "4   cable     65\n",
      "\n",
      "\n",
      "\u001b[1mWords in Column 'Review' after TF-IDF transformation:\u001b[0m\n",
      "         Word     Value\n",
      "13681  guitar  0.350344\n",
      "23434    pick  0.264997\n",
      "30464   sound  0.207045\n",
      "12904    good  0.195455\n",
      "13278   great  0.185972\n",
      "\n",
      "\n",
      "\u001b[1mWords in Column 'Summary' after TF-IDF transformation:\u001b[0m\n",
      "        Word     Value\n",
      "1183   great  0.531131\n",
      "1099    good  0.460982\n",
      "2076    pick  0.283938\n",
      "1286  guitar  0.220470\n",
      "364    cable  0.217129\n",
      "\n",
      "\n",
      "\n",
      "Coherence Score (8-topics):  0.3085074511989887\n",
      "\n",
      "Coherence Score (8-topics):  0.573887214693374\n",
      "\n",
      "\n",
      "\u001b[1mTopic modeling for 'Reviews' using LDA:\u001b[0m\n",
      "Thema 1:\n",
      "serf pedalboard distorted echo jet complains shirt basses adequate trim\n",
      "\n",
      "Thema 2:\n",
      "xlr audio splitter keyboard midi behringer rack hinge mike indeed\n",
      "\n",
      "Thema 3:\n",
      "filter mic pop work cable well look great good case\n",
      "\n",
      "Thema 4:\n",
      "pick guitar good string great cable use sound pedal ve\n",
      "\n",
      "Thema 5:\n",
      "crybaby keyboard sd morley midi pink claytons hardly protector mineral\n",
      "\n",
      "\u001b[1mTopic modeling for 'Summaries' using LDA:\u001b[0m\n",
      "Thema 1:\n",
      "love excellent guitar lock classic solid microphone strap nothing les\n",
      "\n",
      "Thema 2:\n",
      "good pick works great work perfect well favorite picks use\n",
      "\n",
      "Thema 3:\n",
      "quality product job good mic great cable price right cool\n",
      "\n",
      "Thema 4:\n",
      "great nice pedal sound awesome string stuff guitar amazing money\n",
      "\n",
      "Thema 5:\n",
      "cable best capo good price great pick boss guitar acoustic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import all required libraries\n",
    "import pandas\n",
    "import re as regex\n",
    "import string\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', quiet=True) # quiet=True oder silent=True suppresses the downloading output\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Import required LDA Libariers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def main():\n",
    "    def remove_unwanted_characters(token):\n",
    "        token = regex.sub(r'[^a-zA-Z]', ' ', token)\n",
    "        return token\n",
    "\n",
    "    def clean_raw_data(text):        \n",
    "        tokens = word_tokenize(text)\n",
    "        # Removing dots, special characters, and emojis \n",
    "        tokens = [remove_unwanted_characters(token) for token in tokens]\n",
    "        \n",
    "        # List of specific words to be removed\n",
    "        unwanted_words = {'one', 'like'} \n",
    "        tokens = [token for token in tokens if token.lower() not in unwanted_words]\n",
    "        \n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]   \n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Conversion to lowercase\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        return tokens\n",
    "\n",
    "    #CSV IMPORT#\n",
    "    raw_data = pandas.read_csv('Musical_instruments_reviews.csv', keep_default_na=False, nrows=1000)\n",
    "    # the parameter keep_default_na=False is important as otherwise libraries have issues with null values\n",
    "    \n",
    "    raw_reviews = raw_data[\"reviewText\"];\n",
    "    raw_summaries = raw_data[\"summary\"];\n",
    "    \n",
    "    cleaned_reviews = [clean_raw_data(raw_review) for raw_review in raw_reviews]\n",
    "    cleaned_summaries = [clean_raw_data(raw_summary) for raw_summary in raw_summaries]\n",
    "    \n",
    "    def generate_bag_of_words(text_list):\n",
    "        # Prepare data > convert to strings\n",
    "        text_strings = [' '.join(tokens) for tokens in text_list]\n",
    "\n",
    "        # Create an instance of CountVectorizer because CountVectorizer expects a string\n",
    "        vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "       \n",
    "        # Apply the vectorizer to the data\n",
    "        text_bow = vectorizer.fit_transform(text_strings)\n",
    "\n",
    "        # Determine word frequency\n",
    "        word_freq = dict(zip(vectorizer.get_feature_names_out(), np.asarray(text_bow.sum(axis=0)).ravel()))\n",
    "        word_freq_sorted = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        return word_freq_sorted\n",
    "        \n",
    "    frequent_reviews = generate_bag_of_words(cleaned_reviews)\n",
    "    frequent_summaries = generate_bag_of_words(cleaned_summaries)\n",
    "    \n",
    "    def bow_dataframe(word_freq_text):\n",
    "        df = pandas.DataFrame(list(word_freq_text.items()), columns=['Word', 'Count'])\n",
    "        # Sorted list with top 5 words\n",
    "        top_5_words = df.sort_values(by='Count', ascending=False).head(5)\n",
    "        return top_5_words\n",
    "\n",
    "    df_reviews = bow_dataframe(frequent_reviews)\n",
    "    print(\"\\033[1mMost common words in reviews:\\033[0m\")\n",
    "    print(df_reviews) # prints the most frequents words of the column reviews\n",
    "    print(\"\\n\") # adds a break\n",
    "\n",
    "    df_summaries = bow_dataframe(frequent_summaries)\n",
    "    print(\"\\033[1mMost common words in summaries:\\033[0m\")\n",
    "    print(df_summaries) #  prints the most frequents words of the column summaries\n",
    "    print(\"\\n\") # adds a break\n",
    "\n",
    "    def tfidf_vectorize_as_single_document(token_lists):\n",
    "        \n",
    "        # Convert token lists into strings and combine them into a single document\n",
    "        combined_text = ' '.join([' '.join(tokens) for tokens in token_lists])\n",
    "\n",
    "        # Creation and application of the TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "        tfidf_matrix = vectorizer.fit_transform([combined_text])\n",
    "\n",
    "        # Convert the TF-IDF matrix into a DataFrame\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "        return tfidf_df\n",
    "\n",
    "    review_tfidf_df = tfidf_vectorize_as_single_document(cleaned_reviews)\n",
    "    summary_tfidf_df = tfidf_vectorize_as_single_document(cleaned_summaries)\n",
    "\n",
    "    def transform_and_sort_tfidf(tfidf_df):\n",
    "        # Transformation of the DataFrame\n",
    "        transformed_df = tfidf_df.T.reset_index()\n",
    "        transformed_df.columns = ['Word', 'Value']\n",
    "\n",
    "        # Sorting the DataFrame in descending order by 'Value'\n",
    "        sorted_df = transformed_df.sort_values(by='Value', ascending=False)\n",
    "\n",
    "        return sorted_df\n",
    "\n",
    "    sorted_review_tfidf = transform_and_sort_tfidf(review_tfidf_df)\n",
    "    print(\"\\033[1mWords in Column 'Review' after TF-IDF transformation:\\033[0m\")\n",
    "    print(sorted_review_tfidf.head())\n",
    "    print(\"\\n\") # adds a break\n",
    "\n",
    "    sorted_summary_tfidf = transform_and_sort_tfidf(summary_tfidf_df)\n",
    "    print(\"\\033[1mWords in Column 'Summary' after TF-IDF transformation:\\033[0m\")\n",
    "    print(sorted_summary_tfidf.head())\n",
    "    print(\"\\n\") # adds a break\n",
    "\n",
    "    ## Gensim - LDA Coherence Score\n",
    "    def run_lda_analysis(text_list, num_topics):\n",
    "        # Create a Gensim dictionary\n",
    "        dictionary = corpora.Dictionary(text_list)  \n",
    "\n",
    "        # Create a Gensim Corpus\n",
    "        corpus = [dictionary.doc2bow(text) for text in text_list]\n",
    "        \n",
    "        # Train the LDA model\n",
    "        lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, random_state=100, passes=10)\n",
    "        \n",
    "        # Calculation of the Coherence Score\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=text_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        print(f'\\nCoherence Score ({num_topics}-topics): ', coherence_lda)\n",
    "        return lda_model, coherence_lda\n",
    "\n",
    "    # calculation coherence score for column 'Reviews' with 8 topics\n",
    "    lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 8)\n",
    "\n",
    "    # calculation coherence score for column 'Summaries' with 8 topics\n",
    "    lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 8)\n",
    "\n",
    "    # Results from review (took too long on every execution of the program)\n",
    "    #lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 5) 0.2471651210039798\n",
    "    #lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 6) 0.2799919362448972\n",
    "    #lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 7) 0.2910895543971476\n",
    "    #lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 8) 0.3085074511989887\n",
    "    #lda_model_review, coherence_review = run_lda_analysis(cleaned_reviews, 9) 0.29556273834970986\n",
    "\n",
    "    # Results from summary (took too long on every execution of the program)\n",
    "    #lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 5) 0.5329306042849031\n",
    "    #lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 6) 0.5360185919104373\n",
    "    #lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 7) 0.5505435500322134\n",
    "    #lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 8) 0.573887214693374\n",
    "    #lda_model_summary, coherence_summary = run_lda_analysis(cleaned_summaries, 9) 0.5465979486599257\n",
    "\n",
    "    ## LDA Review + Summary\n",
    "    def perform_lda(text_list, n_topics=5, n_top_words=10):\n",
    "        # Create and apply the TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        text_tfidf = vectorizer.fit_transform(text_list)\n",
    "\n",
    "        # Create and apply the LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(text_tfidf)\n",
    "\n",
    "        # Display the top words for each topic\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            print(f\"Thema {topic_idx + 1}:\")\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "            print()\n",
    "\n",
    "    review_strings = [' '.join(cleaned_review) for cleaned_review in cleaned_reviews]\n",
    "    summary_strings = [' '.join(cleaned_summary) for cleaned_summary in cleaned_summaries]\n",
    "\n",
    "    print(\"\\n\") # adds a break\n",
    "    print(\"\\033[1mTopic modeling for 'Reviews' using LDA:\\033[0m\")\n",
    "    perform_lda(review_strings)\n",
    "    print(\"\\033[1mTopic modeling for 'Summaries' using LDA:\\033[0m\")\n",
    "    perform_lda(summary_strings)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8438410-365d-4c2d-9c08-4b1980de96bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
